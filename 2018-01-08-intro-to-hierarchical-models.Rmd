---
title: "Hierarchical Modeling in R: When, why, and how"
subtitle: "RDojo Spring 2018"
author: Joe Hoover
output: 
  html_notebook:
    toc: true
---


# Introduction

This notebook is supposed to serve as a brief, applied introduction to hierarchical linear modeling (HLM) in R. You certainly cannot not learn anything close to everything you *should* know about HLM in an hour; however, this will hopefully get you started or at least provide a foundation for getting started. 

To motivate this workshop, we'll be analyzing real data. As you'll see, this data is a textbook example of why HLM is such a useful tool. 

## Environment setup 

Before we get started, let's make sure we have all requisite packages installed. We'll do that as well as add in a few special ingredients that are worth knowing:

* Distributing package installation across multiple cores
* `require` + `pacman`

```{r, echo=T}
# the detectCores function returns how many cores your computer has
# We'll divide that number by two (i.e. half your cores)
n_cores <- parallel::detectCores()/2

# The Ncpus option tells R how many cores to use when installing packages.
# By distributing the installation process across multiple cores, we can really speed it up

getOption("Ncpus", n_cores)

# Calling require(`package name`) returns TRUE if the package is installed and FALSE
# if it is not installed. Here, we say: if `pacman` is not installed, install it. 
if(require(pacman)==F){
  install.packages('pacman')
}
library(pacman)
p_load(lme4, dplyr, tidyr, ggplot2)

```

## Data overview

The data we'll be analyzing was generated by an online survey. The purpose of this survey was to test the hypothesis that a person can better recall the syntactic structure in a written prompt when it is characterized by syntactical patterns that are more similar to the syntactical patterns that that person tends to use. 

Roughly put, the idea is that different people might have different syntax templates, so to speak, and that it is easier for them to encode and recall stimuli that are more similar to those syntax templates.

So, to generate data for testing this hypothesis, participants (N = 337) were asked to write an essay. 

They then read 9 written prompts, which were each drawn from one of three groups of prompts with distinct syntactical structures. Each of these three groups contained only 3 prompts, thus participants all read the same 9 prompts. Prompts were presented in randomized order.

Then, after a distraction (filler) task, they were asked to reproduce each of the prompts to the best of their ability. 

After the data was collected, the *syntax similarity* between each participant's essay and the 9 prompts was calculated. This metric, which we'll refer to as essay similarity indicates the syntactic alignment between a participant's essay and each prompt. 

Similarly, the syntax similarity between each prompt and a given participant's recollection of that prompt was also calculated. We'll refer to this metric as recall; it indicates the fidelity with which participants recalled the syntactic structure of the prompt. 


Let's go ahead and load the data.

```{r, echo=T}

# R has a load() and save() function that stores R objects (pretty much anything!)
# in  a special R format. They are very convenient and fast. 
# You can wrap load() with get() in order to choose the object name
# that you want to assign to the file you're loading
dat <- get(load('rdojo-2-1-18-dat1.Rda'))

summary(dat)

```

# Modeling 

We are interested in the association between `recall` and `essay`. That is, when the syntax in a person's essay better matched the syntax in a prompt, were they then better able to recreate the syntax of that prompt. 

## Visualization I

We'll get to trying to answer this question with a model in a moment. First, let's try to get a feel for what's going on in the data my generating some graphs. 

To do that, we'll use the package plotting2. Plotting is an incredibly powerful and elegant tool for generating graphs; however, it takes a little while to get used to. In general, it approaches graph generation in terms of *layers*. That is, you build a graph by adding layers (sometimes lots of them) on top of layers. 



Let's see how that works...

```{r}

p1 <- ggplot(data=dat, # Tell ggplot where to find our data
       aes(x=essay, y=recall)) + # the aes() function stands for aesthetics
  geom_point() # to add alayer (called a geom) you simply add it (`+`) to the object

p1
```

Well, how does that look?

ggplot makes it really easy to get a more precise idea of what's going on. Using the geom_smooth() function, we can quickly add a regression line to the plot.

```{r}
p1 + geom_smooth(method='lm')
```

What can we learn from this plot...
* Is there anything going on?
* Any potential issues?


## Correlation

Perhaps the easiest way to test our hypothesis would simply be to estimate the correlation between `recall` and `essay`

```{r}
cor.test(dat$recall, dat$essay, na.rm=T)
```

**What do these results indicate?**  

We could just as easily have estimated a linear regression: 

$recall = \alpha + \beta*essay + \epsilon$

This model states that `recall` is a function of an intercept $\alpha$, `essay`, and residual noise `epsilon`. Further, the effect of `essay` on `recall` is represented as $\beta$, our regression coefficient. 

In R, it's easy to code this up: 

```{r}
m.lm1 <- lm(essay ~ recall, data=dat)
summary(m.lm1)
```

What is this model telling us?

We can also use the sjPlot package (which leverages ggplot2) to easily plot the estimated regression model against the data. 


```{r}
sjp.lm(m.lm1)
```


So, can we stop here? What have we found? Can we trust it?  


## Hierarchical modeling: why

One of the assumptions made by OLS linear regression is that the data is independently and identically distributed (IID). This means that any one data point is not dependent (i.e. associated with, correlated with...) on any other data point *and* that each data point is drawn from the same distribution (identical). 

**Q:** Is this a fair assumption for our data?


**Q:** How might our data violate the IID assumption?



### Clusters & dependence 

The problem with our data is that we should expect some data points to be dependent (i.e. correlated with other data points). For example, recall responses for the same $user_{1}$  should be more similar on average than recall responses from $user_{1}$ and $user_{2}$. 

So we have a potential for clustering at the *user* level.

Responses for specific prompts could also be correlated. For example, maybe one item is just extremely hard to recall, in that case, everyone would perform poorly on it. Further, given that prompts for 3 groups of 3, we might expect that responses for prompts from the same group *also* could be correlated. 

Thus, we also have a potential for clustering at the *prompt* and *cluster* level. 

### Visualizing hierarchical data

We can get a better idea of what this could mean for our data and model through visualization. To do this, we'll look at how the effect of `essay` on `recall` varies across cluster types. 

*Specifically, we'll look at how the intercept and the slope vary across clusters*



```{r}

  ggplot(data=dat, aes(x = essay, y = recall, color=clusterID)) + # define bottom layer
  geom_point() + # add points
  geom_smooth(method='lm') + # add regression lines
  facet_grid(clusterID~.) +  # split the plot into a grid by clusterID
  ggtitle('cassimEssay x cassimRecall') # add a title
  
```



```{r}

  dat$clusterID <- as.factor(dat$clusterID) # We need to convert clusterID from numeric to factor

  ggplot(data=dat, aes(x = essay, y = recall, color=clusterID)) + 
  geom_point() + 
  geom_smooth(method='lm', color='black') +
  facet_grid(clusterID~.) + 
  ggtitle('cassimEssay x cassimRecall')
  
```

What do we see?

Let's do the same thing for `userID` and `promptID`.



```{r}

  dat$promptID <- as.factor(dat$promptID) # We need to convert clusterID from numeric to factor

  ggplot(data=dat, aes(x = essay, y = recall)) + 
  geom_point(color='black') + 
  geom_smooth(method='lm', aes(color=promptID)) +
  #facet_grid(promptID~.) + 
  ggtitle('cassimEssay x cassimRecall')
  
```
We can also easily show the clusterID as a function of `linetype`: 

```{r}
  ggplot(data=dat, aes(x = essay, y = recall)) + 
  geom_point(color='black', alpha=.1) + 
  geom_smooth(method='lm', aes(color=promptID, linetype=clusterID), se=F, size=1.4) +
  #facet_grid(promptID~.) + 
  ggtitle('cassimEssay x cassimRecall')
```



```{r}

  dat$userID <- as.factor(dat$userID) # We need to convert clusterID from numeric to factor

  ggplot(data=dat, aes(x = essay, y = recall)) + 
  geom_point(color='black', alpha=.05) + 
  geom_smooth(method='lm', aes(group=userID), se=F, color='black') +
  #facet_grid(promptID~.) + 
  ggtitle('cassimEssay x cassimRecall')
  
```


What the hell is that! Clearly:

* Users have different intercepts
* Users have different slopes
* Users have different ranges of essay!

## Hierarchical modeling: how

We've established that our data is definitely not IID.  But, what do we do about? Fortunately, it is extremely easy to fit a hierarchical model in R. First, however we have to figure out what model we want to fit!

Remember, our first regression model had the following form: 

$recall = \alpha + \beta*essay + \epsilon$

This model assumed the same intercept $\alpha$ for everyone, but we know this isn't true. It would be much better to relax this assumption. E.g., specify that each person has their own intercept. We can write this down as a formula using subscripts.

$recall_{ij} = \alpha_{00} + \beta*essay + \tau_{0j} + \epsilon_{ij}$

Where:

$recall_{ij}$ = recall for item $i$ nested in person $j$  
$\alpha_{00}$ = the *grand* intercept
$\tau_{0j}$ = the mean deviation for person $j$ from $\alpha_{00}$. I.e.  $\alpha_{00} + \tau_{0j}$ = the intercept for $user_j$

In this model, we're saying that a response to a given item varies as a function of some overall intercept, the effect of essay, a mean adjustment for each person and residual error. 

We can fit this model in R using the `lme4` package: 

```{r}
m.lmer1 <- lmer(recall ~ 1 + essay + (1 | userID), data=dat)
summary(m.lmer1)
```

Here, we can see the estimated effect of essay (and the grand intercept) listed under fixed effects. Under random effects, we see how the userID intercept varies overall. For example, SD = 0.06. This tells us that the distribution of user intercepts varies around a mean intercept of 0.48 (the fixed effect estimate) with a standard deviation of 0.06. 

We can also estimate how much variance is explained by the userID grouping. This metric is called the `ICC` inter-class correlation coefficient. ICC is simply the variance of the random group effect divided by the total random variance. 


```{r}
icc = 0.004269/(0.008538 + 0.004269)
```
The ICC for group in this model is `r print(paste(round(icc,2)*100, '%', sep=''))`. So, the variation between participants explains 33% of the variance in responses. 

However, we know that there are other grouping structures. For example, we know that responses might be clustered within prompts. Let's write out a reasonable formula to represent this and then model it. All we need to do is add another varying intercept $\tau_{0k}$. 


$recall_{ijk} = \alpha_{00} + \beta*essay + \tau_{0j} + \tau_{0k} + \epsilon_{ijk}$

Where:

$\tau_{0k}$ = is the prompt intercept adjustment. 


We can easily model this: 


```{r}
m.lmer2 <- lmer(recall ~ 1 + essay + (1 | userID) + (1 | promptID), data=dat)
summary(m.lmer2)
```
We also, know, however, that there might be dependence between prompts, too, because they come from clusters of prompts. We can represent this as follows: 

$recall_{ijkl} = \alpha_{00} + \beta*essay + \tau_{0j} + \tau_{0k} + \tau_{0l} \epsilon_{ijkl}$


We can model this, too, using `/` notation.

```{r}

# In the last random effect term, we indicate that prompt is nested in cluster.
m.lmer3 <- lmer(recall ~ 1 + essay + (1 | userID) + (1 | clusterID/promptID), data=dat)

summary(m.lmer3)
```

Let's take a step back and use the very handy `sjPlot` package to visualize all of this.

We'll plot the random effects from m.lmer3. This will give us 3 plot panes, 1 for each random effect grouping variable. They are plotted in order of appearence in the model formula: userID, promptID, clusterID

```{r}
sjp.lmer(m.lmer3)
```


Now

